# Source: rag/charts/llm-service/templates/inference-service.yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-3-2-3b-instruct
  annotations:
    openshift.io/display-name: llama-3-2-3b-instruct
    serving.knative.openshift.io/enablePassthrough: "true"
    sidecar.istio.io/inject: "true"
    sidecar.istio.io/rewriteAppHTTPProbers: "true"
    serving.kserve.io/deploymentMode: RawDeployment
  labels:
    opendatahub.io/dashboard: "true"
    networking.knative.dev/visibility: "cluster-local"
    device: gpu
spec:
  predictor:
    maxReplicas: 1
    minReplicas: 1
    model:
      modelFormat:
        name: vLLM
      name: ""
      resources:
        limits:
          cpu: "2"
          memory: 32Gi
          nvidia.com/gpu: "1"
        requests:
          cpu: "1"
          memory: 16Gi
          nvidia.com/gpu: "1"
      runtime: vllm-serving-runtime-gpu
      args:
      - --model
      - meta-llama/Llama-3.2-3B-Instruct
      - --served-model-name
      - meta-llama/Llama-3.2-3B-Instruct
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "14336"
      - --max-num-seqs
      - "32"
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
